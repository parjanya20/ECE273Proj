{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that all variables named reward actually denote cost\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class SimpleMDP:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.transition_probs = np.random.rand(num_states, num_actions, num_states)\n",
    "        self.transition_probs /= self.transition_probs.sum(axis=2, keepdims=True)\n",
    "        print(self.transition_probs)\n",
    "\n",
    "        self.rewards = np.zeros((num_states, num_actions))+0.5\n",
    "        for i in range(num_states):\n",
    "            for j in range(num_actions):\n",
    "                if i%3 == j:\n",
    "                    self.rewards[i,j] = 1\n",
    "        self.gamma = 0.95  \n",
    "        print(self.rewards)\n",
    "    def get_reward(self, state, action):\n",
    "        return self.rewards[state, action]\n",
    "\n",
    "    def get_transition_probs(self, state, action):\n",
    "        return self.transition_probs[state, action]\n",
    "\n",
    "    def next_state(self, state, action):\n",
    "        return np.random.choice(self.num_states, p=self.transition_probs[state, action])\n",
    "\n",
    "def initialize_policy(num_states, num_actions):\n",
    "    return np.full((num_states, num_actions), 1.0 / num_actions)\n",
    "\n",
    "def compute_bregman_divergence(pi, pi_0):\n",
    "    if pi.ndim == 1:\n",
    "        return np.sum(pi * (np.log(pi) - np.log(pi_0 + 1e-10)))  \n",
    "    else:\n",
    "        return np.sum(pi * (np.log(pi) - np.log(pi_0 + 1e-10)), axis=1)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_q_values(mdp, pi, num_states, num_actions):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    for s in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            next_state_probs = mdp.get_transition_probs(s, a)  \n",
    "            immediate_reward = mdp.get_reward(s, a)  \n",
    "            \n",
    "            expected_future_value = 0\n",
    "            for i in range(len(next_state_probs)):\n",
    "                next_state = i\n",
    "                prob = next_state_probs[i]\n",
    "                expected_future_value += prob * (immediate_reward + mdp.gamma * np.dot(Q[next_state], pi[next_state]))\n",
    "            \n",
    "            Q[s, a] = expected_future_value\n",
    "    return Q\n",
    "\n",
    "\n",
    "def update_policy(pi_k, Q, eta_k, num_states, num_actions, pi_0):\n",
    "    pi_next = np.zeros_like(pi_k)\n",
    "    for s in range(num_states):\n",
    "        def objective(p):\n",
    "            entropy_term = -np.sum(p * np.log(p + 1e-10))  # Adding entropy regularization\n",
    "            divergence = compute_bregman_divergence(p, pi_0[s])\n",
    "            return eta_k * np.dot(Q[s], p) + entropy_term + divergence\n",
    "\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda p: np.sum(p) - 1},\n",
    "            {'type': 'ineq', 'fun': lambda p: p}\n",
    "        ]\n",
    "        initial_p = np.full(num_actions, 1.0 / num_actions)\n",
    "        result = minimize(objective, initial_p, constraints=constraints, method='SLSQP', options={'disp': False})\n",
    "        if not result.success:\n",
    "            raise ValueError(\"Optimization failed: {}\".format(result.message))\n",
    "        pi_next[s] = result.x\n",
    "        #Make all pi_next non-negative and values sum to 1\n",
    "        pi_next[s] = pi_next[s] - min(pi_next[s])\n",
    "        pi_next[s] = pi_next[s]/sum(pi_next[s])\n",
    "\n",
    "    return pi_next\n",
    "\n",
    "\n",
    "def simulate_policy(mdp, pi, num_states, num_actions, timesteps=50):\n",
    "    total_reward = 0\n",
    "    current_state = np.random.randint(num_states)\n",
    "    for _ in range(timesteps):\n",
    "        action = np.random.choice(num_actions, p=pi[current_state])\n",
    "        reward = mdp.get_reward(current_state, action)\n",
    "        total_reward += reward\n",
    "        current_state = mdp.next_state(current_state, action)\n",
    "    return total_reward / timesteps  # Average reward\n",
    "\n",
    "num_states = 5\n",
    "num_actions = 3\n",
    "eta_k = 1e-2\n",
    "num_iterations = 1\n",
    "\n",
    "mdp = SimpleMDP(num_states, num_actions)\n",
    "pi_0 = initialize_policy(num_states, num_actions)\n",
    "pi_k = np.copy(pi_0)\n",
    "Q = compute_q_values(mdp, pi_k, num_states, num_actions)\n",
    "\n",
    "for k in range(num_iterations):\n",
    "    pi_k = update_policy(pi_k, Q, eta_k, num_states, num_actions, pi_0)\n",
    "    print(pi_k)\n",
    "    print(Q)\n",
    "    Q = compute_q_values(mdp, pi_k, num_states, num_actions) \n",
    "    avg_reward = simulate_policy(mdp, pi_k, num_states, num_actions, timesteps=500)\n",
    "    print(f\"Iteration {k + 1}: Average Reward = {avg_reward:.4f}\")\n",
    "\n",
    "print(\"Final policy:\", pi_k)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
